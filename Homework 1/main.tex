\documentclass{article}


\author{Matthias Maile}
\date{March 2022}
\title{Homework 1}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}

% page setup
\usepackage[a4paper, margin=2.5cm]{geometry}

\usepackage{multicol} % multicolumn text

% references
\usepackage{hyperref}

\usepackage{unicode-math}


\begin{document}
\maketitle
\tableofcontents

\section{Round-off Errors And Computer Arithmetic}
\label{sec:Round-off errors and Computer Arithmetic}
Here we are looking at the two formulae
\[
  e^{-5} \approx \sum_{i=0}^n \frac{(-5)^i}{i!}
  \qquad
  e^{-5} = \frac{1}{e^5} \approx \sum_{i=0}^n \frac{1}{\frac{5^i}{i!}}.
\]
As it can be seen in \autoref{fig:abs_error1}, Formula 2 is converges much faster 
to the exact value. This is because the sum in Formula 1 is not uniform and has 
large jumps for small $i$.

\begin{figure}[H]
	\centering
	\includegraphics{build/Ex1.pdf}
	\caption{Plot of the absolute error for different $n$.}
	\label{fig:abs_error1}
\end{figure}

\section{Algorithms And Convergence}
\label{sec:Algorithms and Convergence}
The absolute error is plotted in \autoref{fig:abs_error2}. To calculate the degree of each formula, so 
$p$ such that
\[
  \vert \gamma_n - \gamma_inf \vert = K \frac{1}{n^p},
\]
I utilized the curve\_fit from the scipy.optimize library. I defined a function
\[
  f(n, K, p) = \frac{K}{n^p}
\]
so that the curve\_fit method could find optimal $K$ and $p$. This gave the results
\begin{align}
  K_1 &= 0.498 & p_1 &= 0.999, \\
  K_2 &= 0.697 \pm 0.089 & p_2 &= 0.74 \pm 0.023.
\end{align}
So we can tell that the first formula is of order $p\approx1$ and formula 2 $p\approx \frac{3}{4}$.

\begin{figure}[H]
	\centering
	\includegraphics{build/Ex2.pdf}
	\caption{Plot of the absolute error and the fitted curve for different $n$.}
	\label{fig:abs_error2}
\end{figure}


\section{Generating Random Number With Power Law Distribution}
\label{sec:Generating Random Number With Power Law Distribution}
This part consists of two parts: Firstly, we have to derive a function $f$ such that $y = f(x)$ follows
a power law $P(y) = A \cdot y^{-\alpha}, y \in [2, \inf\}$ when $x$ is evenly distrubuted along $(0,1]$. 
In the second part we will implement it in python and test the generated distribution.

\subsection{Deriving $f(x)$}
For $f(x)$ we invert the comulative distribution function (CDF) from $y$. Firstly, we will simplify $P(y)$ 
by finding $A(\alpha$ (they are not independent because of the normalization condition):
\begin{align*}
  \int_{-\infty}^{\infty} P(y) \, \symup{d}y 
  &= \int_{2}^{\infty} P(y) \, \symup{d}y \\
  &= \int_{2}^{\infty} A y^{-\alpha} \, \symup{d}y \\
  &= \frac{-A}{-\alpha+1} \left. y^{-\alpha+1} \right\vert_2^\infty \\
  &= \frac{A}{\alpha - 1} 2^{1-\alpha} \overset{!}{=} 1 \\
  \Leftrightarrow
  A &= (\alpha - 1) 2^{\alpha - 1}
\end{align*}
Now we are ready to compute the CDF of $P(y)$:
\begin{align*}
  \symup{CDF}(y) &= \int_{-\infty}^{y} P(y^\prime) \, \symup{d}y^\prime  \\
                 &= \int_{2}^{y} (\alpha - 1) 2^{\alpha -1} y^{\prime-\alpha} \, \symup{d}y^\prime \\
                 &= (\alpha-1) 2^{\alpha-1} \cdot \frac{1}{-\alpha+1} \left. y^{\prime -\alpha +1} \right\vert_2^y \\
                 &= -2^{\alpha-1} \left[ y^{-\alpha+1} - 2^{-\alpha +1} \right] \\
                 &= 1 - \left( \frac{y}{2} \right)^{1-\alpha}.
\end{align*}
Finally we can invert it to find $y(x)$:
\begin{align*}
  \symup{CDF}(y) &= x \\
  x &= 1 - \left( \frac{y}{2} \right)^{1-\alpha} \\
  \Leftrightarrow 
  1 - x &= \left( \frac{y}{2} \right)^{1-\alpha} \\
  \Leftrightarrow
  2 \cdot \left(1 - x\right)^{\frac{1}{1-\alpha}} = y = f(x).
\end{align*}

\subsection{Implementation and Tests}
\label{sec:implementation}
To test the transformation 
\[
  f(x) = 2 \cdot \left(1 - x\right)^{\frac{1}{1-\alpha}}
\]
we will generate $10000$ samples of $x$ with the numpy.random.rand function and apply $f$ to all of them. For
$\alpha$ we chose the value 5.
The matplotlib.pyplot.hist function will sort those into 50 bins. 

To test the power-law-like behavior we will proceed like in \autoref{sec:Algorithms and Convergence} with 
the curve\_fit from the scipy.optimize library. Since we have intervals, we first compute the middle point
of each interval and than fit the function
\[
  p(x, A, \alpha) = A \cdot x^{-\alpha}
\]
to the data pairs (midpoint, occurences). This gives us a fit parameter 
\[
  \alpha = 5.24 \pm 0.017,
\]
so a deviation of about $4.8\%$ from the true $\alpha = 5$. The data histogram and the curve fit are plotted in 
\autoref{fig:ex3}.

As suggested in the exercise, there is also a log-log-plot. One visible feature of a power law is the linear-like
slope in this plot which should be $\approx \alpha$. This can be either confirmed by looking at 
\autoref{fig:ex3-log} or taking our measured $\alpha$, which corresponds to the actual log-log-slope.
\begin{figure}[H]
	\centering
	\includegraphics{build/Ex3.pdf}
	\caption{Generated powerlaw distribution and measured exponent for $\alpha = 5$, $10000$ samples.}
  \label{fig:ex3}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics{build/Ex3-log.pdf}
	\caption{Double logarithmic plot of the generated data $y = f(x)$ with 10000 samples.}
  \label{fig:ex3-log}
\end{figure}



\section{Trajectory Of A Random Walk}
\label{sec:Trajectory Of A Random Walk}

\subsection{Normal Walker}
\label{sec:Normal Walker}
In this part we take a look at the normal walker, i.e. a walker that can return.
In \autoref{fig:trace-returning} the trace for eight returning walkers is plotted. 
To test the order of the distance after some amount of steps, we simulate 1000 walkers
for 1000 steps each and compute the distance from the origin for each step and take the average.
The result can be seen in \autoref{fig:dist-returning}. One key feature is that the function 
Distance(steps) has the same slop in the log-log-plot as a function $\sqrt{\text{steps}}$, which means that
$R(t) \sim \sqrt{t}$.
\begin{figure}[H]
	\centering
	\includegraphics{build/many_walks.pdf}
	\caption{Eight random walker`s trace plotted for 1000 steps.}
  \label{fig:trace-returning}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics{build/Steps-Distance.pdf}
  \caption{A random walkers distance to the origin after $n$ steps, compared to $\sqrt{t}$ in a log-log-plot. 
  Notice that $\sqrt{t}$ and the distance data have the same slope, so have the same exponential behavior.}
  \label{fig:dist-returning}
\end{figure}

\subsection{Self-avoiding walk (SAW)}
\label{sec:Normal Walker}
For a self avoiding walker we modified our existing program to check all former locations if it has been at the new proposed location before. If the script offered 10 illegal moves in a row it will assume that the walker stuck and
will set all coming positions to the last one. After about 100 steps most walkers were terminating in our
experiments. In \autoref{fig:trace-SAW} eight traces are shown. The distance can be analyzed as well. In 
\autoref{fig:distance-saw} the distance of the walker has been plotted as before; for a better comparison the 
normal walker has been added to the plot as well. Here it can be seen that after about 100 steps most walkers 
terminated already because the average distance does not got further.

Before 30 steps however, the distance $R(t)$ is growing much faster in case of a SAW. A curve fit delivered the
result 
\[
  R \sim t^\frac{1}{d} \qquad \text{with } d = 1.6314.
\]
The parameter $d$ is called the diffusion dimension.
\begin{figure}
	\centering
	\includegraphics{build/many_walks-SAW.pdf}
	\caption{Eight Self-avoiding random walker`s trace plotted for 1000 steps. Note that most walkers terminate 
           before because they are stuck in a position.}
  \label{fig:trace-SAW}
\end{figure}
\begin{figure}
	\centering
	\includegraphics{build/Steps-Distance-SAW.pdf}
  \caption{Distance from origin plotted for a self avoiding and normal walker. For the curve fit the first, fast 
            growing part has been used as marked in the plot.}
  \label{fig:distance-saw}
\end{figure}


\end{document}
